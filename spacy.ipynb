{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy import displacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We're no strangers to love. You know the rules and so do I. A full commitment's what I'm thinking of. You wouldn't get this from any other guy. I just wanna tell you how I'm  feeling. Gotta make you understand. Never gonna give you up. Never gonna let you down. Never gonna run around and desert you. Never gonna make you cry. Never gonna say goodbye. Never gonna tell a lie and hurt you\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = text.replace('.','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We, 're, no, strangers, to, love, You, know, the, rules, and, so, do, I, A, full, commitment, 's, what, I, 'm, thinking, of, You, would, n't, get, this, from, any, other, guy, I, just, wanna, tell, you, how, I, 'm,  , feeling, Got, ta, make, you, understand, Never, gon, na, give, you, up, Never, gon, na, let, you, down, Never, gon, na, run, around, and, desert, you, Never, gon, na, make, you, cry, Never, gon, na, say, goodbye, Never, gon, na, tell, a, lie, and, hurt, you, "
     ]
    }
   ],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "tokens = tokenizer(text2)\n",
    "\n",
    "\n",
    "\n",
    "default = []\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, end = ', ')\n",
    "    default.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blank Tokenizer with English Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're, no, strangers, to, love, You, know, the, rules, and, so, do, I, A, full, commitment's, what, I'm, thinking, of, You, wouldn't, get, this, from, any, other, guy, I, just, wanna, tell, you, how, I'm,  , feeling, Gotta, make, you, understand, Never, gonna, give, you, up, Never, gonna, let, you, down, Never, gonna, run, around, and, desert, you, Never, gonna, make, you, cry, Never, gonna, say, goodbye, Never, gonna, tell, a, lie, and, hurt, you, "
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "# Blank tokenizer with English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokens = tokenizer(text2)\n",
    "\n",
    "blank = []\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, end = ', ')\n",
    "    blank.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[We, 're, no, strangers, to, love, You, know, the, rules, and, so, do, I, A, full, commitment, 's, what, I, 'm, thinking, of, You, would, n't, get, this, from, any, other, guy, I, just, wanna, tell, you, how, I, 'm,  , feeling, Got, ta, make, you, understand, Never, gon, na, give, you, up, Never, gon, na, let, you, down, Never, gon, na, run, around, and, desert, you, Never, gon, na, make, you, cry, Never, gon, na, say, goodbye, Never, gon, na, tell, a, lie, and, hurt, you]\n",
      "[We're, no, strangers, to, love, You, know, the, rules, and, so, do, I, A, full, commitment's, what, I'm, thinking, of, You, wouldn't, get, this, from, any, other, guy, I, just, wanna, tell, you, how, I'm,  , feeling, Gotta, make, you, understand, Never, gonna, give, you, up, Never, gonna, let, you, down, Never, gonna, run, around, and, desert, you, Never, gonna, make, you, cry, Never, gonna, say, goodbye, Never, gonna, tell, a, lie, and, hurt, you]\n"
     ]
    }
   ],
   "source": [
    "print(default)\n",
    "print(blank)\n",
    "\n",
    "## Notice the differences between both tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We PRON\n",
      "'re AUX\n",
      "no DET\n",
      "strangers NOUN\n",
      "to PART\n",
      "love VERB\n",
      ". PUNCT\n",
      "You PRON\n",
      "know VERB\n",
      "the DET\n",
      "rules NOUN\n",
      "and CCONJ\n",
      "so ADV\n",
      "do VERB\n",
      "I. PROPN\n",
      "A DET\n",
      "full ADJ\n",
      "commitment NOUN\n",
      "'s AUX\n",
      "what PRON\n",
      "I PRON\n",
      "'m AUX\n",
      "thinking VERB\n",
      "of ADP\n",
      ". PUNCT\n",
      "You PRON\n",
      "would AUX\n",
      "n't PART\n",
      "get VERB\n",
      "this PRON\n",
      "from ADP\n",
      "any DET\n",
      "other ADJ\n",
      "guy NOUN\n",
      ". PUNCT\n",
      "I PRON\n",
      "just ADV\n",
      "wanna PROPN\n",
      "tell VERB\n",
      "you PRON\n",
      "how SCONJ\n",
      "I PRON\n",
      "'m AUX\n",
      "  SPACE\n",
      "feeling VERB\n",
      ". PUNCT\n",
      "Got VERB\n",
      "ta PART\n",
      "make VERB\n",
      "you PRON\n",
      "understand VERB\n",
      ". PUNCT\n",
      "Never ADV\n",
      "gon VERB\n",
      "na PART\n",
      "give VERB\n",
      "you PRON\n",
      "up ADP\n",
      ". PUNCT\n",
      "Never ADV\n",
      "gon VERB\n",
      "na PART\n",
      "let VERB\n",
      "you PRON\n",
      "down ADP\n",
      ". PUNCT\n",
      "Never ADV\n",
      "gon VERB\n",
      "na PART\n",
      "run VERB\n",
      "around ADV\n",
      "and CCONJ\n",
      "desert VERB\n",
      "you PRON\n",
      ". PUNCT\n",
      "Never ADV\n",
      "gon VERB\n",
      "na PART\n",
      "make VERB\n",
      "you PRON\n",
      "cry VERB\n",
      ". PUNCT\n",
      "Never ADV\n",
      "gon VERB\n",
      "na PART\n",
      "say VERB\n",
      "goodbye NOUN\n",
      ". PUNCT\n",
      "Never ADV\n",
      "gon VERB\n",
      "na PART\n",
      "tell VERB\n",
      "a DET\n",
      "lie NOUN\n",
      "and CCONJ\n",
      "hurt VERB\n",
      "you PRON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print (token,token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only identify verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['love', 'know', 'do', 'thinking', 'get', 'tell', 'feeling', 'Got', 'make', 'understand', 'gon', 'give', 'gon', 'let', 'gon', 'run', 'desert', 'gon', 'make', 'cry', 'gon', 'say', 'gon', 'tell', 'hurt']\n"
     ]
    }
   ],
   "source": [
    "print(\"Verbs:\", [token.text for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization: Reducing different forms of a word to a singular form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We we\n",
      "'re be\n",
      "no no\n",
      "strangers stranger\n",
      "to to\n",
      "love love\n",
      ". .\n",
      "You you\n",
      "know know\n",
      "the the\n",
      "rules rule\n",
      "and and\n",
      "so so\n",
      "do do\n",
      "I. I.\n",
      "A a\n",
      "full full\n",
      "commitment commitment\n",
      "'s be\n",
      "what what\n",
      "I I\n",
      "'m be\n",
      "thinking think\n",
      "of of\n",
      ". .\n",
      "You you\n",
      "would would\n",
      "n't not\n",
      "get get\n",
      "this this\n",
      "from from\n",
      "any any\n",
      "other other\n",
      "guy guy\n",
      ". .\n",
      "I I\n",
      "just just\n",
      "wanna wanna\n",
      "tell tell\n",
      "you you\n",
      "how how\n",
      "I I\n",
      "'m be\n",
      "   \n",
      "feeling feel\n",
      ". .\n",
      "Got got\n",
      "ta to\n",
      "make make\n",
      "you you\n",
      "understand understand\n",
      ". .\n",
      "Never never\n",
      "gon go\n",
      "na to\n",
      "give give\n",
      "you you\n",
      "up up\n",
      ". .\n",
      "Never never\n",
      "gon go\n",
      "na to\n",
      "let let\n",
      "you you\n",
      "down down\n",
      ". .\n",
      "Never never\n",
      "gon go\n",
      "na to\n",
      "run run\n",
      "around around\n",
      "and and\n",
      "desert desert\n",
      "you you\n",
      ". .\n",
      "Never never\n",
      "gon go\n",
      "na to\n",
      "make make\n",
      "you you\n",
      "cry cry\n",
      ". .\n",
      "Never never\n",
      "gon go\n",
      "na to\n",
      "say say\n",
      "goodbye goodbye\n",
      ". .\n",
      "Never never\n",
      "gon go\n",
      "na to\n",
      "tell tell\n",
      "a a\n",
      "lie lie\n",
      "and and\n",
      "hurt hurt\n",
      "you you\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'John, who is due for his first big break in a while, the former NFL running back has continued to impress all staff with his speed and quickness. He started off camp with two touchdown pass attempts in his rookie season, but ran into a number of injuries, including one that kept him from playing one game until the second game from Week 10 against Pittsburgh.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy model\n",
    "lg = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = lg(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", who is due for his \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " big break in a while, the former \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NFL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " running back has continued to impress all staff with his speed and quickness. He started off camp with \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " touchdown pass attempts in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    his rookie season\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", but ran into a number of injuries, including one that kept him from playing \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " game until the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    second\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " game from \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Week 10\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " against \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pittsburgh\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = 'ent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('dp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e081d3b5f60c0ebbd38959c03c7c2d07307c4ada80f42a10f6aaf293f74d342"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
